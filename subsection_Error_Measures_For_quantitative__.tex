
\subsection{Error Measures}

For quantitative analysis, we used 2 types of  typical error measures 

Root Mean squared error which  not which is on the same scale as the data. since the RMSD is scale dependent, it cannot be used to make comparisons between series that are on different scales.[https://www.otexts.org/fpp/2/5]

Percentage errors have the advantage of being scale-independent, but it overlooked the errors for the small actual values.

\begin{table}[]
\centering
\caption{My caption}
\begin{tabular}{|l|r|r|r|r|r|r|r|r|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Dataset}} & \multicolumn{2}{c|}{ARIMA} & \multicolumn{2}{c|}{Exp} & \multicolumn{2}{c|}{Nnet} & \multicolumn{2}{c|}{Current} \\ \cline{2-9} 
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{MSE} & \multicolumn{1}{c|}{MAPE} & \multicolumn{1}{c|}{MSE} & \multicolumn{1}{c|}{MAPE} & \multicolumn{1}{c|}{MSE} & \multicolumn{1}{c|}{MAPE} & \multicolumn{1}{c|}{MSE} & \multicolumn{1}{c|}{MAPE} \\ \hline
airline & 1,412.9442 & 0.7207 & \textbf{1,370.4743} & \textbf{0.4352} & 2,400.5715 & 0.7184 & 1,377.0172 & 0.9246 \\ \hline
sunspotarea & \textbf{382.3602} & 1.3586 & 505.7503 & 1.1607 & 473.9241 & \textbf{0.4652} & 546.9379 & 0.9651 \\ \hline
euretail & \textbf{0.5244} & \textbf{0.0042} & 0.5755 & 0.0107 & 1.8821 & 0.0062 & 0.6503 & 0.0049 \\ \hline
oil & 55.3126 & 0.2508 & 54.9890 & 0.2508 & \textbf{51.6162} & \textbf{0.1595} & 61.8069 & 0.5846 \\ \hline
\end{tabular}
\end{table}


The highlighted cell represents the minimum RMSE and MAPE. According to the results obtained, there is no an individual method which perform well in all the cases. Each method will fit the datasets which satisfies its assumptions well . If the conditions are not met, performance of some methods are below the average. For example neural network perform well on ‘oil’ dataset but perform worse in ‘euretail’.  Current stratos prediction method has the highest mean errors in almost all the cases.

major conclusions of this analysis that there is no single method which perform best in all the case. on the other hand a method which performs well in some dataset, might perform worst in another dataset.
Since an PaaS autonomous autoscaler should be able to expect any type of workload pattern, the prediction method should be able to give good enough estimates in average case while without without making a large error on specific patterns.

Idea of ensemble learning is quite frequently used in situations where there is no dominant technique which can provide the best results but can get good enough results from combining the results from several weak learners. In general forecasting domain we can see several models combining techniques  proposed in various researches [Combining time series models for forecasting], [Intelligent techniques for forecasting multiple time series in real-world systems ],
[Time series forecasting using a hybrid ARIMA and neural network model ].  

There are various views regarding model selection and combining multiple models. Some researches says combined methods improve the accuracy while some researches says it reduce the probability of getting larger errors than increasing the accuracy.According to the literature there are multiple ways of combining the individual results 
\begin{itemize}
\item In the simple average, all models are assigned equal weights, i.e. wi=1⁄n (i=1, 2,..., n) [9, 10].
\end{itemize}
\begin{itemize}
\item In the trimmed average, individual forecasts are combined by a simple arithmetic mean, excluding the worst performing k% of the models. A trimming of 10%–30% is usually recommended [9, 10].
\end{itemize}
\begin{itemize}
\item In the Winsorized average, the i smallest and i largest forecasts are selected and respectively set as the (i+1)th smallest and (i+1)th largest forecasts [9].
\end{itemize}
\begin{itemize}
\item In the median-based combining, the combination function f is the median of the individual forecasts. Median is sometimes preferred over simple average as it is less sensitive to extreme values [12, 13].
\end{itemize}
\begin{itemize}
\item In the error-based combining, the weight to each model is assigned to be the inverse of the past forecast error (e.g. MSE, MAE, MAPE, etc.) of the corresponding model [3, 10].
\end{itemize}
\begin{itemize}
\item In the variance-based method, the optimal weights are determined through the minimization of the total Sum of Squared Error (SSE) [7, 10].
\end{itemize}
