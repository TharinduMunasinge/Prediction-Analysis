\subsection{Error Measures}
For quantitative analysis, we used 2 types of error measures:

\begin{itemize}
\item \textit{Root mean squared errors (RMSE)} are of the same scale as the data. since it is scale dependent, it cannot be used to make comparisons between series that are of different scales. \cite{Forecasting_OTexts}

$$RMSE=\sqrt{\frac{\sum_{t=1}^{n}(\hat{x}_{t}-x_{t})^{2}}{n}}$$
\item \textit{Percentage errors} have the advantage of being scale-independent, but they overemphasize the errors for values that are actually small.
$$MAPE=\frac{1}{n}\sum_{t=1}^{n}\left | \frac{\hat{x}_{t}-x_{t}}{x_{t}} \right |$$
\end{itemize}


\begin{table}[]
\centering
\caption{Comparison of errors from MSE and MAPE}
\begin{tabular}{|l|l|r|r|r|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Model}} & \multicolumn{2}{c|}{sunspotarea}                     & \multicolumn{2}{c|}{Euretail}                        \\ \cline{2-5} 
\multicolumn{1}{|c|}{}                       & \multicolumn{1}{c|}{MSE} & \multicolumn{1}{c|}{MAPE} & \multicolumn{1}{c|}{MSE} & \multicolumn{1}{c|}{MAPE} \\ \hline
ARIMA                                        & 382.36019                & 1.35863                   & 0.52439                  & 0.00417                   \\ \hline
Exponential                                  & 505.75034                & 1.16073                   & 0.57553                  & 0.01071                   \\ \hline
Neural net.                                         & 473.92407                & 0.46524                   & 1.88208                  & 0.00618                   \\ \hline
Current                                      & 546.93791                & 0.96513                   & 0.65035                  & 0.00391                   \\ \hline
\end{tabular}
\end{table}

\begin{table}[]
\centering
\caption{Comparison of errors from MSE and MAPE}
\label{my-label}
\begin{tabular}{|l|r|r|r|r|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Model}} & \multicolumn{2}{c|}{oil}                             & \multicolumn{2}{c|}{airmiles}                        \\ \cline{2-5} 
\multicolumn{1}{|c|}{}                       & \multicolumn{1}{c|}{MSE} & \multicolumn{1}{c|}{MAPE} & \multicolumn{1}{c|}{MSE} & \multicolumn{1}{c|}{MAPE} \\ \hline
ARIMA                                        & 55.31262                 & 0.25081                   & 1,412.94416              & 0.72068                   \\ \hline
Exponential                                  & 54.98904                 & 0.25078                   & 1,370.47426              & 0.43516                   \\ \hline
Neural net.                                         & 51.61619                 & 0.15951                   & 2,400.57153              & 0.71841                   \\ \hline
Current                                      & 61.80686                 & 0.58458                   & 1,367.01715              & 0.92456                   \\ \hline
\end{tabular}
\end{table}

The highlighted cells represent the minimum RMSE and MAPE. According to the results obtained, there is no individual model which performs perfectly in all cases. Each model will fit the datasets which satisfy its assumptions. Where they are not satisfied, performance of some models would be below the average. For example, neural networks perform well on the ‘oil’ dataset but perform poorly on ‘euretail’. The current prediction method from Stratos has the highest mean errors in almost all the cases.

The analysis primarily depicts that there is no single method which guarantees best performance in all cases. A method which performs well in some dataset might perform worse in another dataset.
Since an autonomous PaaS auto scaler should be able to expect any type of workload pattern, the prediction method should be able to give good enough estimates in an average case without producing large errors on any specific patterns.

The idea of ensemble learning is quite frequently used in situations where there is no dominant technique which can provide the best results, but it is possible to obtain good enough results by combining results from several weak learners. In the general forecasting domain we can see several model combining techniques proposed in various researches \cite{Zou_2004}, \cite{Wagner_2011},
\cite{Zhang_2003}.  

There are various views regarding model selection and combining multiple models. Some researches claim that combined methods improve accuracy, while others claim that they suppress the effects of large errors from individual models rather than improving overall accuracy. According to literature there are multiple ways of combining individual results\cite{Adhikari_2012}:

\begin{itemize}
\item Simple average : all models are assigned equal weights $w_=\frac{1}{n}$.
\item Median-based :  Get the median of the forecasts from each method.
\item Trimmed average: Get the average of forecasts excluding  excluding the worst performing models.
\item Error-based combining: the weight of each model is assigned to be the inverse of the past forecast error(MAE,MAPE,RMSE) of each model.
\item Variance-based method, the optimal weights are determined through the minimization of the total sum of squared error.
\end{itemize}