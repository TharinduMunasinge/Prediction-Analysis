\section{Prediction horizon}

Prediction Horizon
Another interesting point in workload prediction is estimating the prediction interval, r. Islam et al. [22] proposed using a 12-minute interval, because the set-up time of VM instances in the cloud was typically around 5-15 minutes by that time. Time-series forecasting can be combined with reactive techniques. Iqbal et al. [23] proposed a hybrid scaling technique that utilizes reactive rules for scaling up (based on CPU usage) and a regression-based approach for scaling down.

\section{Evaluation of the existing models}

We have chosen 4 widely used prediction techniques in the literature (ARIMA, Neural network, Exponential Models and Naive prediction) and the existing workload prediction technique in Stratos. Then tested on several workloads <<DESCRIBE THOSE LOADS>> and saw that the some prediction technique performs well in some workload patterns while other technique perform well in different workloads.
For evaluation purpose we have used forecast package in R << DETAILS ABOUT THE PACKAGE>>. In that library there are time series datasets in different domains with typical patterns like trends, cycles and seasonality factors. In this analysis we wanted to qualitatively analyse the ability of each prediction technique to learn from current workload history and predict the time series while time series data is evolving periodically as what happen in autonomous autoscaler in operation. So that we have simulated online training scenario by streaming data points of the time series one at a time to each method and get the 1-step lookahead prediction and plotted against the real data points. Since the objective of this experiment to identify  how well an individual method can predict the future values by only considering the past history, we tried several publically available datasets in this package with predictable patterns even though they are not really related to the cloud workloads. The experimental results elaborated in <<IN SUB SECTION 1>>, are based on actual cloud workload datasets (CPU,Memory and Http Request in flight)  in which we  compare and contrast the performance of the proposed method vs existing methods. 
For this evaluation we have used the implementations of the Forecast pacakge[ID] . <<DESCRIPTON ABOUT THE IMPLEMENTATION>>>>
ARIMA: According to <<ROBIN>> auto.arima()  find out the best fitted ARIMA model and approximated the model coeffficents in by minimizing the error<<<BESPECIFIC >>> [https://www.otexts.org/fpp/8/7]

Ets: Model the data with best fitted exponential model [https://www.otexts.org/fpp/7/7]

	Nnetar: [https://www.otexts.org/fpp/9/3]

\subsubsection{Dataset 1: airmiles}
airmiles : The revenue passenger miles flown by commercial airlines in the United States for each year from 1937 to 1960.

\subsubsection{Dataset 2:euretial}
Euretial: Quarterly retail trade index in the Euro area (17 countries), 1996-2011, covering wholesale and retail trade, and repair of motor vehicles and motorcycles. (Index: 2005 = 100).

\subsubsection{Dataset 3:oil}
Annual oil production (millions of tonnes), Saudi Arabia, 1965-2010.

\subsubsection{Dataset 4:sunspot areas}
Annual averages of the daily sunspot areas (in units of millionths of a hemisphere) for the full sun. Sunspots are magnetic regions that appear as dark spots on the surface of the sun. The Royal Greenwich Observatory compiled daily sunspot observations from May 1874 to 1976. Later data are from the US Air Force and the US National Oceanic and Atmospheric Administration.

\subsubsection{Error Measures}

For quantitative analysis, we used 2 types of  typical error measures 

Root Mean squared error which  not which is on the same scale as the data. since the RMSD is scale dependent, it cannot be used to make comparisons between series that are on different scales.[https://www.otexts.org/fpp/2/5]

Percentage errors have the advantage of being scale-independent, but it overlooked the errors for the small actual values.

\begin{table}[]
\centering
\caption{My caption}
\begin{tabular}{|l|r|r|r|r|r|r|r|r|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Dataset}} & \multicolumn{2}{c|}{ARIMA} & \multicolumn{2}{c|}{Exp} & \multicolumn{2}{c|}{Nnet} & \multicolumn{2}{c|}{Current} \\ \cline{2-9} 
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{MSE} & \multicolumn{1}{c|}{MAPE} & \multicolumn{1}{c|}{MSE} & \multicolumn{1}{c|}{MAPE} & \multicolumn{1}{c|}{MSE} & \multicolumn{1}{c|}{MAPE} & \multicolumn{1}{c|}{MSE} & \multicolumn{1}{c|}{MAPE} \\ \hline
airline & 1,412.9442 & 0.7207 & \textbf{1,370.4743} & \textbf{0.4352} & 2,400.5715 & 0.7184 & 1,377.0172 & 0.9246 \\ \hline
sunspotarea & \textbf{382.3602} & 1.3586 & 505.7503 & 1.1607 & 473.9241 & \textbf{0.4652} & 546.9379 & 0.9651 \\ \hline
euretail & \textbf{0.5244} & \textbf{0.0042} & 0.5755 & 0.0107 & 1.8821 & 0.0062 & 0.6503 & 0.0049 \\ \hline
oil & 55.3126 & 0.2508 & 54.9890 & 0.2508 & \textbf{51.6162} & \textbf{0.1595} & 61.8069 & 0.5846 \\ \hline
\end{tabular}
\end{table}


The highlighted cell represents the minimum RMSE and MAPE. According to the results obtained, there is no an individual method which perform well in all the cases. Each method will fit the datasets which satisfies its assumptions well . If the conditions are not met, performance of some methods are below the average. For example neural network perform well on ‘oil’ dataset but perform worse in ‘euretail’.  Current stratos prediction method has the highest mean errors in almost all the cases.

major conclusions of this analysis that there is no single method which perform best in all the case. on the other hand a method which performs well in some dataset, might perform worst in another dataset.
Since an PaaS autonomous autoscaler should be able to expect any type of workload pattern, the prediction method should be able to give good enough estimates in average case while without without making a large error on specific patterns.

Idea of ensemble learning is quite frequently used in situations where there is no dominant technique which can provide the best results but can get good enough results from combining the results from several weak learners. In general forecasting domain we can see several models combining techniques  proposed in various researches [Combining time series models for forecasting], [Intelligent techniques for forecasting multiple time series in real-world systems ],
[Time series forecasting using a hybrid ARIMA and neural network model ].  

There are various views regarding model selection and combining multiple models. Some researches says combined methods improve the accuracy while some researches says it reduce the probability of getting larger errors than increasing the accuracy.According to the literature there are multiple ways of combining the individual results 
In the simple average, all models are assigned equal weights, i.e. wi=1⁄n (i=1, 2,..., n) [9, 10].
In the trimmed average, individual forecasts are combined by a simple arithmetic mean, excluding the worst performing k% of the models. A trimming of 10%–30% is usually recommended [9, 10].
In the Winsorized average, the i smallest and i largest forecasts are selected and respectively set as the (i+1)th smallest and (i+1)th largest forecasts [9].
In the median-based combining, the combination function f is the median of the individual forecasts. Median is sometimes preferred over simple average as it is less sensitive to extreme values [12, 13].
In the error-based combining, the weight to each model is assigned to be the inverse of the past forecast error (e.g. MSE, MAE, MAPE, etc.) of the corresponding model [3, 10].
In the variance-based method, the optimal weights are determined through the minimization of the total Sum of Squared Error (SSE) [7, 10].
