\section{Evaluation of Existing Models}

In this analysis we wanted to qualitatively analyze the ability of individual prediction techniques to learn from current workload history and predict a time series while time series itself is evolving periodically, similar to what happens in case of an autonomous auto-scaler in operation. We simulated an online training scenario by streaming data points of the time series one at a time to each method and plotted the one-step lookahead predictions from the resulting models at each stage, against the real data points. For evaluation purpose we used the forecast package in R \cite{forecastPackage} which contains time series datasets in different domains with typical patterns like trends, cycles and seasonality factors.

\subsection{Datasets}
Since the objective of this experiment is to identify how well an individual prediction method can predict future values by only considering the past history, we used several publicly available datasets with predictable patterns from the forecast package, some of which are not directly related to cloud workloads. Neverthless, the experimental results elaborated in Section 5, where we compare and contrast the performance of the proposed method vs existing methods, are based on two real-world cloud workloads and the Google cluster dataset, in addition to those found in the forecast package.

\begin{itemize}
\item airmiles: The revenue passenger miles flown by commercial airlines in the United States for each year from 1937 to 1960
\item euretail: Quarterly retail trade index in the Euro area (17 countries), 1996-2011, covering wholesale and retail trade, and repair of motor vehicles and motorcycles (Index: 2005 = 100)
\item sunspotarea: Annual averages of the daily sunspot areas (in units of millionths of a hemisphere) for the full sun
\item oil: Annual oil production (millions of tonnes), Saudi Arabia, 1965-2010
\end{itemize}

\subsection{Time series forecasting techniques}
We chose three widely used prediction methods from literature, namely ARIMA, neural network and exponential models, and the existing workload prediction technique in Stratos.

\subsubsection{ARIMA}
This model combines two models called autoregression (of order $p$) and moving average (of order $q$) .\\

\textit{Autoregression model AR($p$):}
In an autoregression model, the variable of interest is forecast using a linear combination of past values of the variable. The term autoregression indicates that it is a regression of the variable against itself. Thus an autoregressive model of order $p$ can be written as
	$$x_t = c + \phi_1x_{ t-1} + \phi_2x_{t-2} +...+ \phi_px_{t-p} + e_{t}$$
where $c$ is a constant and $e_t$ is white noise. Autoregressive models are normally restricted to stationary data, and then some constraints are applied on the values of the parameters \cite{Forecasting_OTexts}. \\
For an AR(1) model: $-1 < \phi_{1} < 1$ \\
For an AR(2) model: $-1 < \phi_{2} < 1$ ,$\phi_{2}+ \phi_{1} < 1$ , $\phi_{2} - \phi_{1} < 1$ \\

\textit{Moving average model MA($q$):}
This model uses past forecast errors in a regression-like model.
	$$x_t =  c + \theta_1e_{ t-1} + \theta_2e_{t-2} +...+ \theta_qe_{t-q}$$
where $e_t$ is white noise. $x_t$ can be thought of as a weighted moving average of the last few forecast errors. However, moving average models should not be confused with moving average smoothing. The former is used for forecasting future values while the latter is used for estimating the trend-cycle of past values.
By combining differencing with autoregression and a moving average model, we obtain a non-seasonal ARIMA model. ARIMA is an acronym for \textit{autoregressive integrated moving average}. The full model can be written as:

	$${x}'_t = c + \phi_1{x}'_{ t-1} + \phi_2{x}'_{t-2} +...+ \phi_p{x}'_{t-p} + e_{t} + \theta_1e_{ t-1} + \theta_2e_{t-2} +...+ \theta_qe_{t-q}$$
The “predictors” on the right hand side include both lagged values of $X_t$ and lagged errors. This is called an ARIMA($p$, $d$, $q$) model, where $p$ and $q$ are the orders of the autoregressive and moving average parts respectively, and $d$ is the degree of first differencing involved.


\subsubsection{Neural networks}
ANNs are a class of nonlinear, nonparametric, data-driven and self-adaptive models, originally inspired by the intelligent working mechanism of human brains. Over the years, ANNs are used as excellent alternative to the common statistical models for time series forecasting. The most popular ANN architectures in the forecasting domain are the multilayer perceptrons (MLPs). They consist of a feed-forward structure of three layers, viz. an input layer, one or more hidden layer and an output layer. The nodes in each layer are connected to those in the immediate next layer by acyclic links . Single hidden layer is sufficient for most applications. It is often customary to use the notation (p, h, q) for referring an ANN with p input, h hidden and q output nodes.

\subsubsection{Exponential model}
\textit{Exponential weighted moving average:}
In exponential weighted moving average methods, forecasts are calculated using weighted averages where the weights decrease exponentially as observations come from further in the past. The smallest weights are associated with the oldest observations \cite{Forecasting_OTexts} \cite{StatSoft} \cite{STAT510}.

\textit{Simple exponential smoothing:}
	$$X_{t+1}=\alpha x_t +\alpha(1-\alpha)X_{t-1}$$
	$$X_{t+1}=\alpha x_t +\alpha(1-\alpha)x_{t-1}+\alpha(1-\alpha)^2x_{t-2}+\alpha(1-\alpha)^3x_{t-3}+...$$
where $0\leq \alpha \leq 1$ is the smoothing parameter. The one-step-ahead forecast for time $t+1$ is a weighted average of all the observations in the series $x_1$, ..., $x_t$. The rate at which the weights decrease is controlled by the parameter $\alpha$. If $\alpha$ is small (i.e. close to 0), more weight is given to observations from the distant past. If $\alpha$ is large (i.e., close to 1), more weight is given to the more recent observations. This method is suitable for forecasting data with no trend or seasonal patterns.

\textit{Double exponential smoothing:}
Double smoothing can be applied to a time-series with an existing linear trend. This can be derived by applying exponential smoothing to the data already smoothed using simple exponential smoothing. There is a need to add a second smoothing constant to account for the trend.

\textit{Triple exponential smoothing:} Triple smoothing can be applied to a time-series with existing trend and seasonality. This can be derived by applying exponential smoothing to the data already smoothed using double exponential smoothing.

\subsubsection{Prediction method in Apache Stratos}
Apache Stratos is an open source PaaS framework where a user can built a PaaS system on existing IaaS. In the auto scaling solution used in Stratos, there is a workload demand prediction method which can be explained using the motion equation in physics. The method calculate one lookahead prediction using the workload demand in last period.
$$S_{t+h|t} = u_{t}h+ \frac{1}{2} a_{t}h^2 \\
\hat{x}_{t+h}=avg(x)_{t}  + S_{t+h|t}$$ \\

\begin{table}[]
\centering
\caption{List of symbols}
\label{my-label}
\begin{tabular}{ll}
Parameter       & Description                                                                     \\
$S_{t+h|t}$     & The predicted change of the workload for time $(t+h)$ based on the interval $t$ \\
$x$             & Workload metrics                                                                \\
$\hat{x}_{t+h}$ & Predicted workload metrics for time $t+h$                                       \\
$u_{t}$         & First derivative of the workload metrics within the $t^{th}$ interval           \\
$a_{t}$         & Second derivative of the workload metrics within the $t^{th}$ interval          \\
$h$             & time horizon                                                                   
\end{tabular}
\end{table}


\subsection{Time series forecasting library}

We have used the implementations of the time series forecasting models in R forecast package  \cite{forecastPackage}.

$auto.arima()$:  finds out the best fitted ARIMA model and approximates the model coefficients by minimizing the past forecast error \cite{Forecasting_OTexts}.

$ets()$: finds out the best fitted exponential model among the family of exponential models (simple exponential smoothing, Holt's linear trend, exponential trend, Holt-Winters seasonal method) automatically and calculates the model coefficients by reducing the past forecast error. \cite{Forecasting_OTexts}
    
$nnetar()$: feed-forward neural networks with a single hidden layer and lagged inputs for forecasting univariate time series