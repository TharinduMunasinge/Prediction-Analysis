\subsection{Observations}

Figures 1 to 4 show the comparison of each of the time series prediction models under four datasets. Respective error measures are presented in Table 2 and 3. The highlighted cells represent the minimum RMSE and MAPE values. 

According to the results obtained, there is no single model which performs well in all online training scenarios. Each model will fit the datasets which satisfy its assumptions. Where they are not satisfied, performance of some models would be below the average. For example, while neural networks perform well on the \textit{oil} dataset, they perform poorly on \textit{euretail} dataset. The current prediction method from Stratos has the highest mean errors almost all the cases considered.

\begin{table}[]
\centering
\caption{Comparison of errors from RMSE and MAPE}
\begin{tabular}{|l|l|r|r|r|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Model}} & \multicolumn{2}{c|}{sunspotarea}                     & \multicolumn{2}{c|}{Euretail}                        \\ \cline{2-5}
\multicolumn{1}{|c|}{}                       & \multicolumn{1}{c|}{RMSE} & \multicolumn{1}{c|}{MAPE} & \multicolumn{1}{c|}{RMSE} & \multicolumn{1}{c|}{MAPE} \\ \hline
ARIMA                                        & 382.360                & 1.359                   & 0.524                  & 0.004                   \\ \hline
Exponential                                  & 505.750                & 1.161                   & 0.576                  & 0.011                   \\ \hline
Neural net.                                  & 473.924                & 0.465                   & 1.882                  & 0.006                   \\ \hline
Current                                      & 546.938                & 0.965                   & 0.650                  & 0.004                   \\ \hline
\end{tabular}
\end{table}

\begin{table}[]
\centering
\caption{Comparison of errors from RMSE and MAPE}
\label{my-label}
\begin{tabular}{|l|r|r|r|r|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Model}} & \multicolumn{2}{c|}{oil}                             & \multicolumn{2}{c|}{airmiles}                        \\ \cline{2-5} 
\multicolumn{1}{|c|}{}                       & \multicolumn{1}{c|}{RMSE} & \multicolumn{1}{c|}{MAPE} & \multicolumn{1}{c|}{RMSE} & \multicolumn{1}{c|}{MAPE} \\ \hline
ARIMA                                        & 55.313                 & 0.251                   & 1,412.944              & 0.721                   \\ \hline
Exponential                                  & 54.989                 & 0.251                   & 1,370.474              & 0.435                   \\ \hline
Neural net.                                  & 51.616                 & 0.159                   & 2,400.572              & 0.718                   \\ \hline
Current                                      & 61.807                 & 0.585                   & 1,367.017              & 0.925                   \\ \hline
\end{tabular}
\end{table}

The analysis primarily depicts that there is no single method which guarantees best performance in all cases. A method which performs well in some dataset might perform worse in another dataset. As an autonomous PaaS auto-scaler should be able to work with any type of workload pattern, the prediction method should be able to give good enough estimates in an average case without producing large errors on any specific workload pattern.

The idea of ensemble learning is quite frequently used in situations where there is no dominant technique which can provide the best results, but it is possible to obtain good enough results by combining results from several weak learners. In the general forecasting domain we can see several models combining techniques proposed by several researchers \cite{Wagner_2011}, \cite{Zhang_2003}, \cite{Zou_2004}.

There are several views regarding model selection and combining multiple models. While some researchers claim that combined methods improve accuracy, others claim that they suppress the effects of large errors from individual models rather than improving the overall accuracy. According to the literature, there are multiple ways of combining individual results \cite{Adhikari_2012}:

\begin{itemize}
\item Simple average -- Obtaining the average of the forecast from each model as the forecast of ensemble.
\item Median-based -- Obtaining the median of the forecast from each model as the forecast of ensemble.
\item Trimmed average -- Obtaining the average of forecasts while excluding the worst performing models.
\item Error-based combining -- Assigning the weight of each model to be the inverse of the past forecast error (e.g., MAE, MAPE, or RMSE) of each model.
\item Variance-based method -- Determining the optimal weights through minimization of the total sum of squared error.
\end{itemize}