\section{Proposed Ensemble Prediction Method}

We propose an error based ensemble technique for workload prediction. Contrary to what is mentioned in Section 2, offline training is not possible in this problem since the workload history data may not be available at the beginning of the prediction process. While the auto scaler is in operation, workload history gets accumulated based on the user's workload requirement (CPU, memory, and request count). However, the prediction method should still be able to predict the future horizon based on the initially available dataset. After initial predictions, actual data will become available for the next time periods, so that we can accumulate the latest actual data into the workload history and use them for the next forecast horizon.

In the existing error based combining techniques, mean values of error metrics (absolute error, absolute percentage error, squared error, etc.) are taken into account when calculating the contribution factors for individual methods. 

Considering the currently available dataset in the time series as
$X=[x_{1},x_{2},.. x_{t}]$
we want to calculate predictions for $x_{t+h}$. Consider the final predicted value as $\hat{x}_{(t+h)}$ and the predicted value from the $i$\textsuperscript{th} model for $x_{t+h}$ as $\hat{x}_{(t+h)}^{(i)}$. We define $\hat{x}_{(t+h)}$ as a weighted sum of predictions from each model:
$$\hat{x}_{(t+h)}= \sum_{i=1}^{k}w_i \hat{x}_{(t+h)}^{(i)}$$   $$\forall k \in \{1,2,3,...,n\}$$

Here, $w_i$ is the weight assigned to the $i$\textsuperscript{th} forecasting method. To ensure unbiasedness, it is often assumed that the weights add up to unity \cite{Adhikari_2012}. Hence we define the contribution from the $i$\textsuperscript{th} model to the final result as $c_i$, so that the same equation can be redefined as  
$$\hat{x}_{(t+h)}= \frac{\sum_{i=1}^{k}c_i \hat{x}_{(t+h)}^{(i)}}{\sum_{i=1}^{k}c_i}$$
here $w_{i}= \frac{c_{i}}{\sum_{j=1}^{k}c_j}$.

Since $\sum_{j=1}^{k}w_{j}=1$, this weight assignment is unbiased and would result in the weighted average of the predictions.

\subsection{Determination of the Contribution coefficients}

To determine the optimal contribution coefficients in our proposed ensemble technique, we use past forecast errors for each model. In our prediction problem, accuracy of the prediction for the next time horizon is more significant. Hence the contributions are calculated using inverses of the past forecasting error measures. There are various choices for error measurement, and we have identified certain pros and cons of each measure.

\subsubsection{Error of the most recent prediction}

This model does not capture overall accuracy; it only captures how accurately the last prediction has been calculated by the model. if the model has repeatedly made large errors in past predictions except the most recent one, contribution coefficients are biased and may lead to error-prone decisions.

Absolute error of the last prediction
$$AE_t=|\hat{x}_t-x_t|$$

Squared error of the last prediction
$$SE_t=(\hat{x}_t-x_t)^2$$
penalizes errors more efficiently than the absolute error.

Absolute percentage/relative error
$$APE_t=100\left |\frac{\hat{x}_t-x_t}{x_t}  \right |$$
is sensitive to errors in small actual values.

Average error over history captures the overall error in past predictions, where all past errors have the same level of significance. If a method produces a larger error in older predictions, the contribution is reduced significantly even though it produces the best predictions for most recent values.

Mean absolute error/root mean squared error
$$RMSE=\sqrt{\frac{\sum_{t=1}^{n}(\hat{x}_{t}-x_{t})^{2}}{n}}$$ 

Mean absolute error/ Root mean squared error
$$ MAPE=\frac{1}{n}\sum_{t=1}^{n}\left | \frac{\hat{x}_{t}-x_{t}}{x_{t}} \right |$$

When calculating contributions based on errors, models whose errors are based on the last observation overlook overall accuracy and assign high importance to the last prediction error. On the other extreme, average errors assume that all the last prediction errors have the same level of significance. However, what we really need is an error measure which has larger contribution from the errors in more recent predictions while having smaller contribution from early predictions. Exponential smoothing provides the best fit for our requirement, so we can calculate the contribution as the errors are fitted under the exponential model. Contribution coefficients $c_{i,t}$ are calculated from the inverse of the the fitted past forecast errors ($e_{t}$). If $b_{i,t}$ is the fitted value of the past forecasting error from $i$\textsuperscript{th} model at $t$\textsuperscript{th} time interval, 
$c_{i,t}=\frac{1}{b_{i,t}}$. $e_{t}$ can be the absolute error, squared error or absolute relative error at $t$\textsuperscript{th} prediction.

\begin{multline}
b_{i,t}= \alpha e_t + (1-\alpha)b_{i,t-1} \ where \ 0\leq \alpha \leq 1	\\ 
b_{i,t}=\alpha e_t + \alpha(1-\alpha)e_{t-1}+\alpha(1-\alpha)^2e_{t-1}+\alpha(1-\alpha)^3e_{t-1}+ . .. \  \\
 c_{i,t}=\frac{1}{b_{i,t}}
\end{multline}

\subsubsection{Prediction algorithm}

\begin{enumerate}
\item Consider the time series history window at time $t$, $X=[x_{1},x_{2},.. x_{t}]$.
\item Calculate forecast value for $h$, horizon from the $i$\textsuperscript{th} method, $\hat{x}_{(t+h)}^{(i)} \forall i \in \{1,2,3,...,k\}$.
\item Fit the fitted values using $i$\textsuperscript{th} method for the last $t$ window using an exponential smoothing model and calculate the contribution factor from $i$\textsuperscript{th} model at $t$, $c_(i,t)$.
\item Calculate the point forecast for time $t+h$ using 
$\hat{x}_{(t+h)}= \frac{\sum_{i=1}^{k}c_{(i,t)} \hat{x}_{(t+h)}^{(i)}}{\sum_{i=1}^{k}c_{(i,t)}}$.
\item At time $t+1$, actual value for time $t+1$ will be obtained. Add this value to the history window $X=X\cup \{x_{t+1}\}$ and repeat from step 2.
\end{enumerate}