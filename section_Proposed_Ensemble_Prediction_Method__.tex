\section{Proposed Ensemble Prediction Method}

We propose an error based ensemble technique for workload prediction. Contrary to what is mentioned in Section 2, offline training is not possible in this problem since the workload history data may not be available at the beginning of the prediction process. While the auto scaler is in operation, workload history gets accumulated based on the user's workload requirement (CPU, memory, and request count). However, the prediction method should still be able to predict the future horizon based on the initially available dataset. After initial predictions, actual data will become available for the next time periods, so that we can accumulate the latest actual data into the workload history and use them for the next forecast horizon.

In the existing error based combining techniques, mean values of error metrics (absolute error, absolute percentage error, squared error, etc.) are taken into account when calculating the contribution factors for individual methods. 

Considering the currently available dataset in the time series as
$X=[x_{1},x_{2},.. x_{t}]$
we want to calculate predictions for $x_{t+h}$. Consider the final predicted value as $\hat{x}_{(t+h)}$ and the predicted value from the $i$\textsuperscript{th} model for $x_{t+h}$ as $\hat{x}_{(t+h)}^{(i)}$. We define $\hat{x}_{(t+h)}$ as a weighted sum of predictions from each model:
$$\hat{x}_{(t+h)}= \sum_{i=1}^{k}w_i \hat{x}_{(t+h)}^{(i)} \ \ \forall k \in \{1,2,3,...,n\}$$

Here, $w_i$ is the weight assigned to the $i$\textsuperscript{th} forecasting method. To ensure unbiasedness, it is often assumed that the weights add up to unity \cite{Adhikari_2012}. Hence we define the contribution from the $i$\textsuperscript{th} model to the final result as $c_i$, so that the same equation can be redefined as  
$$\hat{x}_{(t+h)}= \frac{\sum_{i=1}^{k}c_i \hat{x}_{(t+h)}^{(i)}}{\sum_{i=1}^{k}c_i}$$
here $w_{i}= \frac{c_{i}}{\sum_{j=1}^{k}c_j}$.

Since $\sum_{j=1}^{k}w_{j}=1$, this weight assignment is unbiased and would result in the weighted average of the predictions.

\subsection{Determination of the Contribution coefficients}

To determine the optimal contribution coefficients in our proposed ensemble technique, we use past forecast errors for each model. In our prediction problem, accuracy of the prediction for the next time horizon is more significant. Hence the contributions are calculated using inverses of the past forecasting error measures. There are various choices for error measurement, and we have identified certain pros and cons of each measure.

\subsubsection{Error of the most recent prediction}

This model does not capture overall accuracy; it only captures how accurately the last prediction has been calculated by the model. if the model has repeatedly made large errors in past predictions except the most recent one, contribution coefficients are biased and may lead to error-prone decisions.

\textit{Absolute error of the last prediction:}
$$AE_t=|\hat{x}_t-x_t|$$

\textit{Squared error of the last prediction:}
$$SE_t=(\hat{x}_t-x_t)^2$$
penalizes errors more efficiently than the absolute error.

\textit{Absolute percentage/relative error:}
$$APE_t=100\left |\frac{\hat{x}_t-x_t}{x_t}  \right |$$
is sensitive to errors in small actual values.

\subsubsection{Average error over prediction history}
This captures the overall error in past predictions, where all past errors have the same level of significance. If a method has produced larger errors in older predictions, the contribution may be reduced significantly even though it may be producing the best predictions for more recent values.

\textit{Mean absolute error/root mean squared error:}
$$RMSE=\sqrt{\frac{\sum_{t=1}^{n}(\hat{x}_{t}-x_{t})^{2}}{n}}$$ 

\textit{Mean absolute error/ Root mean squared error:}
$$MAPE=\frac{1}{n}\sum_{t=1}^{n}\left | \frac{\hat{x}_{t}-x_{t}}{x_{t}} \right |$$

When calculating contributions based on errors, models whose errors are based on the last observation overlook overall accuracy and assign high importance to the last prediction error. On the other extreme, average errors assume that all the last prediction errors have the same level of significance. However, what we really need is an error measure which has a larger contribution from the errors in more recent predictions and smaller contributions from early predictions. Exponential smoothing provides the best fit for our requirement, so we can calculate the contribution as the errors are fitted under the exponential model. Here, contribution coefficients $c_{i,t}$ are calculated from the inverses of the the fitted past forecast errors ($e_{t}$). If $b_{i,t}$ is the fitted value of the past forecasting error from the $i$\textsuperscript{th} model at the $t$\textsuperscript{th} time interval, $c_{i,t}=\frac{1}{b_{i,t}}$. $e_{t}$ can be the absolute error, squared error or absolute relative error at $t$\textsuperscript{th} prediction.

\begin{multline}
b_{i,t}= \alpha e_t + (1-\alpha)b_{i,t-1} \ \mathrm{where} \ 0\leq \alpha \leq 1	\\ 
b_{i,t}=\alpha e_t + \alpha(1-\alpha)e_{t-1}+\alpha(1-\alpha)^2e_{t-1}+\alpha(1-\alpha)^3e_{t-1}+ . .. \  \\
 c_{i,t}=\frac{1}{b_{i,t}}
\end{multline}

\subsection{Selection Of Models}

Above error based weighting mechanism can be used with different combinations of forecasting models. To preserve the ability to cope up with drastically different workload patterns, models used in ensemble technique should conceptually be different and capture different relationships within datasets.

ARIMA model pre-assumed linear form of the model.That is, a linear correlation  structure is assumed among the time series values and therefore,  nonlinear patterns can not be captured by the ARIMA model \cite{Zhang_2003}. On the other hand seasonal ARIMA model can fit the seasonal factors of the time series well. 

Neural Network allow complex nonlinear relationships within a time series. It has a data driven approach which can extract the patterns within time series without requiring pre-defined relationships in data. But Nerual Network require sufficiently large amount of data points to identify the patterns. Observations in Section 3 also show that neural network makes larger errors in initial stages due to lack of data points, but perform extremely well after it correctly identify the relationship. Training time of the Neural network is also significant, since we proposes real-time training, there should not be too much data point history which slows down the prediction process.

Contrary to general belief that ARIMA is more general than Exponential models, there is no an equivalent model for some of the non linear exponential models (e.g exponential trend model). Therefore to preserve the  generality, we choose Exponential model as a part of ensemble solution. 

While linear exponential smoothing models are all special cases of ARIMA models, the non-linear exponential smoothing models ( e.g Exponential trend model)  have no equivalent ARIMA counterparts. There are also many ARIMA models that have no exponential smoothing counterparts \cite(AHyndman 2013)

There is a possibility of getting out of range forecasts from above methods specially in neural network in early stage. So that we added a naive forecast in which the last real value is taken as the forecast for next interval, also as part of the complete ensemble model.


\subsubsection{Prediction algorithm}

\begin{enumerate}
\item Consider the time series history window at time $t$, $X=[x_{1},x_{2},.. x_{t}]$.
\item Calculate forecast value from the $i$\textsuperscript{th} method over the horizon $h$, $\hat{x}_{(t+h)}^{(i)} \forall i \in \{1,2,3,...,k\}$.
\item Fit a history window of the last $t$ actual data points using the $i$\textsuperscript{th} method.
\item Use an exponential smoothing model to fit the errors resulting from step 3, and use them to calculate the contribution factor for the $i$\textsuperscript{th} model at $t$, $c_{(i,t)}$.
\item Calculate the point forecast for time $t+h$ using
$\hat{x}_{(t+h)}= \frac{\sum_{i=1}^{k}c_{(i,t)} \hat{x}_{(t+h)}^{(i)}}{\sum_{i=1}^{k}c_{(i,t)}}$.
\item At time $t+1$, actual value for time $t+1$ will be available. Add this value to the history window $X=X\cup \{x_{t+1}\}$ and repeat from step 2.
\end{enumerate}