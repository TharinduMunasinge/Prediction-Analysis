\section{Proposed Ensemble Prediction Method}

We propose an error-based ensemble technique for workload prediction. Our approach tries to address situations where offline training is not possible due to workload history data not being available at the beginning of the prediction process. While the auto scaler is in operation, workload history gets accumulated based on the user's workload requirement (e.g., CPU, memory, and request count). However, the prediction method should still be able to predict the future time horizon based on the initially available dataset. After the initial predictions, actual data will become available for the next time periods, so that we can accumulate the latest actual data into the workload history and use them for the next forecast horizon.

In existing error-based combining techniques, mean values of error metrics (e.g., absolute error, absolute percentage error, squared error, etc.) are taken into account while calculating the contributing factors for individual methods. 

Considering the currently available dataset in the time series as
$X=[x_{1},x_{2},.. x_{t}]$
we want to calculate predictions for $x_{t+h}$. Let the final predicted value be $\hat{x}_{(t+h)}$ and the predicted value from the $i$-th model for $x_{t+h}$ be $\hat{x}_{(t+h)}^{(i)}$. We define $\hat{x}_{(t+h)}$ as a weighted sum of predictions from each model:
$$\hat{x}_{(t+h)}= \sum_{i=1}^{k}w_i \hat{x}_{(t+h)}^{(i)} \ \ \forall k \in \{1,2,3,...,n\}$$

where $w_i$ is the weight assigned to the $i$-th forecasting method. To ensure unbiasedness, it is often assumed that the weights add up to unity \cite{Adhikari_2012}. Hence, we define the contribution from the $i$-th model to the final result as $c_i$, so that the same equation can be redefined as:
$$\hat{x}_{(t+h)}= \frac{\sum_{i=1}^{k}c_i \hat{x}_{(t+h)}^{(i)}}{\sum_{i=1}^{k}c_i}$$ 
where $w_{i}= \frac{c_{i}}{\sum_{j=1}^{k}c_j}$. As $\sum_{j=1}^{k}w_{j}=1$, this weight assignment is unbiased and would result in the weighted average of the predictions.

\subsection{Determination of Contribution Coefficients}
To determine the optimal contribution coefficients in our proposed ensemble technique, we use past forecast errors for each model. In our prediction problem, accuracy of the prediction of the next time horizon is more significant. Hence, the contributions are calculated using inverses of the past forecasting error measures. There are various choices for error measurement, and next we discuss several selected measures.

\subsubsection{Error of the Most Recent Prediction}
This model only captures how accurately the last prediction has been calculated by the model. Hence, it does not capture the overall accuracy. If the model has repeatedly made large errors in past predictions, except the most recent one, contribution coefficients are biased and may lead to error-prone decisions.

\noindent
\textit{Absolute Error of the Last Prediction} can be defined as follows:
$$AE_t=|\hat{x}_t-x_t|$$

\noindent
\textit{Squared error of the Last Prediction}, which penalizes errors more efficiently than the absolute error, can be defined as follows:
$$SE_t=(\hat{x}_t-x_t)^2$$

\noindent
Where as \textit{Absolute Percentage/Relative Error} is sensitive to errors in small actual values and is defined as:
$$APE_t=100\left |\frac{\hat{x}_t-x_t}{x_t}  \right |$$

\subsubsection{Average Error Over Prediction History}
This measure captures the overall error in past predictions, where all past errors have the same level of significance. If a method has produced larger errors in past predictions, the contribution may be reduced significantly even though it may be producing the best predictions for more recent values.

\noindent
In this case \textit{Mean Absolute Error/Root Mean Squared Error} can be defined as:
$$RMSE=\sqrt{\frac{\sum_{t=1}^{n}(\hat{x}_{t}-x_{t})^{2}}{n}}$$ 

\noindent
Whereas \textit{Mean Absolute Percentage Error} is defined as:
$$MAPE=\frac{1}{n}\sum_{t=1}^{n}\left | \frac{\hat{x}_{t}-x_{t}}{x_{t}} \right |$$

While calculating the contributions based on errors, models whose errors are based on the last observation overlook overall accuracy and assign high importance to the last prediction error. On the other extreme, the average errors assume that all the last prediction errors have the same level of significance. However, what we really need is an error measure which has a larger contribution from the errors in more recent predictions and smaller contributions from early predictions. Exponential smoothing provides the best fit for our requirement, so that we can calculate the contribution as the errors are fitted under the exponential model. Here, the contribution coefficients $c_{i,t}$ are calculated from the inverses of the the fitted past forecast errors ($e_{t}$). If $b_{i,t}$ is the fitted value of the past forecasting error from the $i$-th model at the $t$-th time interval, $c_{i,t}=\frac{1}{b_{i,t}}$. $e_{t}$ can be the absolute error, squared error, or absolute relative error at $t$-th prediction. $b_{i,t}$ can be defined as follows:

$$b_{i,t}= \alpha e_t + (1-\alpha)b_{i,t-1}$$
where $0\leq \alpha \leq 1$
$$b_{i,t}=\alpha e_t + \alpha(1-\alpha)e_{t-1}+\alpha(1-\alpha)^2e_{t-1}+\alpha(1-\alpha)^3e_{t-1}+ . .. $$
$$c_{i,t}=\frac{1}{b_{i,t}}$$

\subsection{Selection of Models}
The above mentioned error based, weighting mechanism can be used with different combinations of forecasting models. To preserve the ability to cope up with drastically different workload patterns, models used in ensemble techniques should conceptually be different and capture the different characteristics of datasets.

ARIMA assumes a linear form of the model, i.e., a linear correlation structure is assumed among the time series values. Therefore, it cannot capture non-linear patterns \cite{Zhang_2003}. Alternatively, seasonal ARIMA models can fit the seasonal factors of the time series accurately.

A neural network can capture complex nonlinear relationships within a time series. It has a data driven approach which can extract patterns within a time series without predefined knowledge on the relationships inside data. But neural networks require sufficiently large amounts of data points to accurately identify patterns. Observations in Section 3 also show that neural networks make larger errors in initial stages due to lack of data, but perform well after they correctly identify the relationships. Time to train a neural networks is also significant factor. As we propose real-time training, the data point history should not be allowed to grow arbitrarily large, which would slow down the prediction process.

Contrary to the general belief that ARIMA is more general than exponential models, there are no equivalent ARIMA models for some of the nonlinear exponential models (e.g., the exponential trend model) \cite{AHyndman 2013}. Therefore, to preserve generality, we also choose the exponential model as part of our ensemble solution.

There is a possibility that the above models may produce out-of-range forecasts, especially in the case of early-stage neural networks. We compensate this with a naive forecast model, which reproduces the last data point as a forecast for the next interval, added as part of the complete ensemble model.


\subsection{Proposed Prediction Algorithm}
Proposed workload prediction technique can be explained using the following algorithm:
\begin{enumerate}
\item Consider the time series history window at time $t$, $X=[x_{1},x_{2},.. x_{t}]$.
\item Calculate forecast value from the $i$-th time series forecasting method over the time horizon $h$, $\hat{x}_{(t+h)}^{(i)} \forall i \in \{1,2,3,...,k\}$, where $k$ is the number of forecasting methods used.
\item Fit a history window for the last $t$ actual data points using the $i$-th method.
\item Use an exponential smoothing model to fit the errors resulting from Step 3, and use them to calculate the contribution factor for the $i$-th model at $t$, $c_{(i,t)}$.
\item Calculate the point forecast for time $t+h$ using
$\hat{x}_{(t+h)}= \frac{\sum_{i=1}^{k}c_{(i,t)} \hat{x}_{(t+h)}^{(i)}}{\sum_{i=1}^{k}c_{(i,t)}}$.
\item At time $t+1$, actual value for time $t+1$ will be available. Add this value to the history window $X=X\cup \{x_{t+1}\}$ and repeat from Step 2.
\end{enumerate}